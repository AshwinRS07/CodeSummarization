# -*- coding: utf-8 -*-
"""DL-Code_Summarization.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TSAS1thT3tSqeFIqZLSPPSolJp1gP8iv

Setup
"""

!nvidia-smi

#!pip install transformers
!pip install torch==1.4.0
import torch
#since later versions don't seem to have SAVE_STATE_WARNING
! pip install -q transformers==3.5.0 fast-trees
#import fast_trees

!git clone -q https://github.com/microsoft/CodeXGLUE.git

"""Get Java code-summary pairs from the CodeSearchNet dataset"""

!wget -q https://s3.amazonaws.com/code-search-net/CodeSearchNet/v2/java.zip
!unzip -qq java.zip

"""Use helper functions provided by the CodeSearchNet repository to conduct data exploration"""

from pathlib import Path
from typing import List,Optional

!cd sample_data/

!ls

import pandas as pd
import numpy as np

def jsonl_list_to_dataframe(file_list, columns=['code','docstring']):
    """Load a list of jsonl.gz files into a pandas DataFrame."""
    return pd.concat([pd.read_json(f, 
                                   orient='records', 
                                   compression='gzip',
                                   lines=True)[columns] 
                      for f in file_list], sort=False)
def get_dfs(path: Path) -> List[pd.DataFrame]:
    """Grabs the different data splits and converts them into dataframes"""
    dfs = []
    for split in ["train", "valid", "test"]:
        files = sorted((path/split).glob("**/*.gz"))
        df = jsonl_list_to_dataframe(files).rename(columns = {'code': 'mthd', 'docstring': 'cmt'})
        dfs.append(df)
        
    return dfs

path = Path('.')
print(path)
df_train, df_val, df_test = get_dfs(path/'java/final/jsonl')

df_train = df_train.sample(frac = 0.02)
df_val = df_val.sample(frac = 0.02)
df_test = df_test.sample(frac = 0.02)

print(len(df_train),len(df_val),len(df_test))

df_train.head()

"""Cleaning
1. Remove Non-ASCII characters
2. Check for outdated or non-synchronous code-string pairs
3. Remove bad comments, determined if length of comment is more than the code itself, since it probablly has extra information not useful for the model
4. Remove HTML Tags and code tags
"""

def ascii_check(s):
  try:
    s.encode(encoding='utf-8').decode('ascii')
  except UnicodeDecodeError:
    return False
  else:
    return True
df_train = df_train[df_train['mthd'].apply(lambda x: ascii_check(x))]
df_val = df_val[df_val['mthd'].apply(lambda x: ascii_check(x))]
df_test = df_test[df_test['mthd'].apply(lambda x: ascii_check(x))]

df_train = df_train[df_train['cmt'].apply(lambda x: ascii_check(x))]
df_val = df_val[df_val['cmt'].apply(lambda x: ascii_check(x))]
df_test = df_test[df_test['cmt'].apply(lambda x: ascii_check(x))]

!pip install javalang==0.13.0
import javalang

import re
from fast_trees.core import FastParser

parser = javalang.parser

def get_comment_param(cmt:str) -> List[str]:
  params = re.findall('@param+\s+\w+', cmt)
  param_names = []
  for param in param_names:
    param_names.append(param.split()[1])
  return params

def check_outdated(mthd,cmt,parser):
  try:
    method_params = (parser.parse(mthd))[1]
    print(method_params)
  except:
    return False
  comment_params = get_comment_param(cmt)
  return method_params != comment_params

df_train = df_train[~df_train.apply(lambda x:check_outdated(x.mthd,x.cmt,parser),axis = 1)]
df_val = df_val[~df_val.apply(lambda x:check_outdated(x.mthd,x.cmt,parser),axis = 1)]
df_test = df_test[~df_test.apply(lambda x:check_outdated(x.mthd,x.cmt,parser),axis = 1)]

print(len(df_train), len(df_val),len(df_test))

#print(len(df_train1), len(df_val1),len(df_test1))

#print(df_train==df_train1)

"""3."""

df_train = df_train[df_train.apply(lambda row: len(row.mthd) > len(row.cmt), axis = 1)]
df_val = df_val[df_val.apply(lambda row: len(row.mthd) > len(row.cmt), axis = 1)]
df_test = df_test[df_test.apply(lambda row: len(row.mthd) > len(row.cmt), axis = 1)]

len(df_train), len(df_val), len(df_test)

def has_code(cmt):
    if '<code>' in cmt: return True
    else: return False

df_train = df_train[~df_train['cmt'].apply(lambda x: has_code(x))]
df_val = df_val[~df_val['cmt'].apply(lambda x: has_code(x))]
df_test = df_test[~df_test['cmt'].apply(lambda x: has_code(x))]

len(df_train), len(df_val), len(df_test)

import tqdm

def remove_jdocs(df):
    '''
    Remove the JavaDocs leaving only the description of the comment

    :param df: the pandas dataframe to remove the JavaDocs from
    :returns: a new pandas dataframe with the JavaDocs removed
    '''
    methods = []
    comments = []
    for i, row in tqdm.tqdm(list(df.iterrows())):
        comment = row["cmt"]
        # Remove {} text in comments from https://stackoverflow.com/questions/14596884/remove-text-between-and-in-python/14598135
        comment = re.sub("([\{\[]).*?([\)\}])", '', comment)
        
        
        cleaned = []
        for line in comment.split('\n'):
            if "@" in line: break
            cleaned.append(line)
        comments.append('\n'.join(cleaned))
        methods.append(row["mthd"])
    new_df = pd.DataFrame(zip(methods, comments), columns = ["mthd", "cmt"])

    return new_df

df_train = remove_jdocs(df_train)
df_val = remove_jdocs(df_val)
df_test = remove_jdocs(df_test)

def clean_html(cmt):
    result = re.sub(r"<.?span[^>]*>|<.?code[^>]*>|<.?p[^>]*>|<.?hr[^>]*>|<.?h[1-3][^>]*>|<.?a[^>]*>|<.?b[^>]*>|<.?blockquote[^>]*>|<.?del[^>]*>|<.?dd[^>]*>|<.?dl[^>]*>|<.?dt[^>]*>|<.?em[^>]*>|<.?i[^>]*>|<.?img[^>]*>|<.?kbd[^>]*>|<.?li[^>]*>|<.?ol[^>]*>|<.?pre[^>]*>|<.?s[^>]*>|<.?sup[^>]*>|<.?sub[^>]*>|<.?strong[^>]*>|<.?strike[^>]*>|<.?ul[^>]*>|<.?br[^>]*>", "", cmt)
    return result

df_train.cmt = df_train.cmt.apply(clean_html)
df_val.cmt = df_val.cmt.apply(clean_html)
df_test.cmt = df_test.cmt.apply(clean_html)

df_train = df_train.applymap(lambda x: ' '.join(x.split()).lower())
df_val = df_val.applymap(lambda x: ' '.join(x.split()).lower())
df_tst = df_test.applymap(lambda x: ' '.join(x.split()).lower())

df_trn = df_train[~(df_train['cmt'] == '')]
df_val = df_val[~(df_val['cmt'] == '')]
df_test = df_test[~(df_test['cmt'] == '')]

df_train = df_train[~df_train['cmt'].duplicated()]
df_val = df_val[~df_val['cmt'].duplicated()]
df_test = df_test[~df_test['cmt'].duplicated()]

len(df_train), len(df_val), len(df_test)

df_train.head()

"""Exploration"""

from collections import Counter
from transformers import AutoTokenizer
from statistics import mean,median,stdev

def get_counter(df,tokenizer,col):
  tokens = []
  for i,row in df.iterrows():
    tokens.extend(tokenizer.tokenize(row[col]))
  count = Counter()
  for token in tokens:
    count[token] += 1
  return count

tokenizer = AutoTokenizer.from_pretrained('microsoft/codebert-base')
method_count = get_counter(df_train,tokenizer,'mthd')
comment_count = get_counter(df_train,tokenizer,'cmt')
method_len = df_train.mthd.apply(lambda x: len(tokenizer.tokenize(x))).values
comment_len = df_train.cmt.apply(lambda x: len(tokenizer.tokenize(x))).values
max_method_len = int(np.quantile(method_len,0.9))
max_comment_len = int(np.quantile(comment_len,0.9))

import matplotlib.pyplot as plt

def plot(counts,top):
  label,val = zip(*counts.most_common()[:top])
  idx = np.arange(len(label))
  plt.figure(num=None,figsize=(22,4),dpi = 60, facecolor='w',edgecolor='k')
  plt.bar(idx,val,1)
  plt.xticks(idx+0.5,label)
  plt.show()

plot(method_count,top=30)
plot(comment_count,top=30)

def plot_histogram(lens,n_bins=50):
  n,bins,patches = plt.hist(lens,n_bins,alpha=0.9)
print('mean:',mean(method_len),'median:',median(method_len),'stdev',stdev(method_len))
plot_histogram(method_len)

print('mean:',mean(comment_len),'median:',median(comment_len),'stdev',stdev(comment_len))
plot_histogram(comment_len)

"""Training"""

cd ./CodeXGLUE/Code-Text/code-to-text/code

!mkdir java

import json

df_train['code_tokens'] = df_train.mthd.apply(lambda x:x.split())
df_train['docstring_tokens'] = df_train.cmt.apply(lambda x:x.split())
with open('java/train.jsonl','w') as f:
  for i, row in df_train.iterrows():
    f.write(json.dumps(row.to_dict())+'\n')
df_val['code_tokens'] = df_val.mthd.apply(lambda x:x.split())
df_val['docstring_tokens'] = df_val.cmt.apply(lambda x:x.split())
with open('java/valid.jsonl','w') as f:
  for i, row in df_val.iterrows():
    f.write(json.dumps(row.to_dict())+'\n')
df_test['code_tokens'] = df_test.mthd.apply(lambda x:x.split())
df_test['docstring_tokens'] = df_test.cmt.apply(lambda x:x.split())
with open('java/test.jsonl','w') as f:
  for i, row in df_test.iterrows():
    f.write(json.dumps(row.to_dict())+'\n')

lang = 'java'
lr = 5e-5
batch_size = 16
beam_size = 20
source_l = 256
target_l = max_comment_len
data_dir = '.'
output_dir = f'model/{lang}'
train_file = f'{data_dir}/{lang}/train.jsonl'
dev_file = f'{data_dir}/{lang}/valid.jsonl'
epochs = 100
model = 'microsoft/codebert-base'

!python run.py --do_train --do_eval --model_type roberta --model_name_or_path {model} --train_filename {train_file} --dev_filename {dev_file} \
               --output_dir {output_dir} --max_source_length {source_l} --max_target_length {target_l} --beam_size {beam_size} --train_batch_size {batch_size} \
               --eval_batch_size {batch_size} --learning_rate {lr} --num_train_epochs {epochs}

batch_size = 64
dev_file = f'{data_dir}/{lang}/valid.jsonl'
test_file=f'{data_dir}/{lang}/test.jsonl'
test_model =  f'{output_dir}/checkpoint-best-bleu/pytorch_model.bin'

!python run.py --do_test --model_type roberta --model_name_or_path microsoft/codebert-base --load_model_path {test_model} --dev_filename {dev_file} \
               --test_filename {test_file} --output_dir {output_dir} --max_source_length {source_l} --max_target_length {target_l} --beam_size {beam_size} --eval_batch_size {batch_size}

import torch.nn as nn

from model import Seq2Seq
from transformers import RobertaConfig, RobertaModel

config = RobertaConfig.from_pretrained(model)
encoder = RobertaModel.from_pretrained(model,config=config)
decoder_layer = nn.TransformerDecoderLayer(d_model = config.hidden_size,nhead = config.num_attention_heads)
decoder = nn.TransformerDecoder(decoder_layer,num_layers=6)
model = Seq2Seq(encoder=encoder,decoder=decoder,config=config,beam_size=beam_size,max_length=target_l,sos_id=tokenizer.cls_token_id,eos_id=tokenizer.sep_token_id)
model.load_state_dict(torch.load(Path(output_dir)/'checkpoint-last/pytorch_model.bin'))
model.to('cuda')

idx = 0
text2summarize = df_val.mthd.values[idx]
print('Code:',text2summarize)
print('Original Summary:',df_val.cmt.values[idx])

from run import convert_examples_to_features, Example

class Args:
  max_source_length = source_l
  max_target_length = target_l
args = Args()

def predict(df):
  ps = []
  for i,row in tqdm.tqdm(df.iterrows(),total=len(df)):
    examples = [Example(i,source=row.mthd,target=row.cmt)]
    eval_features = convert_examples_to_features(examples,tokenizer,args,stage = 'test')
    source_ids = torch.tensor(eval_features[0].source_ids,dtype=torch.long).unsqueeze(0).to('cuda')
    source_mask = torch.tensor(eval_features[0].source_mask,dtype=torch.long).unsqueeze(0).to('cuda')
    with torch.no_grad():
      preds = model(source_ids=source_ids,source_mask=source_mask)
      for pred in preds:
        tex = pred[0].cpu().numpy()
        tex=list(tex)
        if 0 in tex:
          tex=tex[:tex.index(0)]
          text = tokenizer.decode(tex,clean_up_tokenization_spaces=False)
          ps.append(text)
  return ps

df_val = df_val.reset_index(drop=True)
preds = predict(df_val.head(20))
for i,row in df_val.head(20).iterrows():
  print('Code:',row.mthd)
  print('Original Summary:',row.cmt)
  print('Output Summary:', preds[i])
  print('='*40)

def predict_losses(df):
  ps,losses = [],[]
  for idx, row in tqdm.tqdm(df.iterrows(), total=len(df)):
    examples = [Example(idx, source = row.mthd, target = row.cmt)]
    eval_features = convert_examples_to_features(examples, tokenizer, args, stage='test')
    source_ids = torch.tensor([f.source_ids for f in eval_features], dtype = torch.long).to('cuda')
    source_mask = torch.tensor([f.source_mask for f in eval_features], dtype = torch.long).to('cuda')
    target_ids = torch.tensor([f.target_ids for f in eval_features], dtype = torch.long).to('cuda')
    target_mask = torch.tensor([f.target_mask for f in eval_features], dtype = torch.long).to('cuda')

    with torch.no_grad():
      _, loss, _ = model(source_ids = source_ids, source_mask = source_mask,target_ids = target_ids, target_mask = target_mask)
      preds = model(source_ids = source_ids, source_mask = source_mask)  
      for pred in preds:
        t = pred[0].cpu().numpy()
        t = list(t)
        if 0 in t:
          t = t[:t.index(0)]
        text = tokenizer.decode(t,clean_up_tokenization_spaces=False)
        ps.append(text)
        losses.append(loss.item())
  return ps, losses

df_head = df_val.copy()
ps,losses = predict_losses(df_head)
df_head['pred'],df_head['loss'] = ps,losses
df_sorted = df_head.sort_values('loss',ascending=False)
for i,row in df_sorted.head(20).iterrows():
  print('Code:',row.mthd)
  print('Original Summary:',row.cmt)
  print('Output Summary:', row.pred)
  print('Loss:',row.loss)
  print('='*40)

plt.plot(losses)

